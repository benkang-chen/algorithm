参考链接： https://blog.csdn.net/u013719780/article/details/78352262；

http://www.hankcs.com/ml/principal-component-analysis-k-means-cs229.html。

## PCA算法介绍

主成分分析（Principal Components Analysis），简称PCA，是一种数据降维技术，用于数据预处理。一般我们获取的原始数据维度都很高，比如1000个特征，在这1000个特征中可能包含了很多无用的信息或者噪声，真正有用的特征才100个，那么我们可以运用PCA算法将1000个特征降到100个特征。这样不仅可以去除无用的噪声，还能减少很大的计算量。

### PCA算法是如何实现的？

简单来说，就是将数据从原始的空间中转换到新的特征空间中，例如原始的空间是三维的(x,y,z)，x、y、z分别是原始空间的三个基，我们可以通过某种方法，用新的坐标系(a,b,c)来表示原始的数据，那么a、b、c就是新的基，它们组成新的特征空间。在新的特征空间中，可能所有的数据在c上的投影都接近于0，即可以忽略，那么我们就可以直接用(a,b)来表示数据，这样数据就从三维的(x,y,z)降到了二维的(a,b)。

### 问题是如何求新的基(a,b,c)?

一般步骤是这样的：先对原始数据零均值化，然后求协方差矩阵，接着对协方差矩阵求特征向量和特征值，这些特征向量组成了新的特征空间。参考链接http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90

## 衡量投影的方法

### 1.最大方差理论

​	在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的k维特征是将n维样本点变换为k维后，每一维上的样本方差都尽可能的大。

​	首先，考虑在一维空间 (k=1) 上的投影。我们可以使用 n 维向量u定义这个空间的方向。为了方便(并且不失一般性)，我们假定选择一个单位向量，从而  $$ u^T u = 1$$ ，（注意，我们只对$$u$$的方向感兴趣，而对 $$u$$本身的大小不感兴趣)。

![](https://github.com/benkang-chen/algorithm/blob/master/algorithm_by_python/picture/pca1.png)

如上图所示，红色点表示原样本点$$ x^{(i)}$$，u是蓝色直线的斜率也是直线的方向向量，而且是单位向量，直线上的蓝色点表示原样本点$$ x^{(i)}$$在u上的投影。容易知道投影点离原点的距离是$$x^{(i)T}u$$ ,由于这些原始样本点的每一维特征均值都为0，因此投影到u上的样本点的均值仍然是0.

假设原始数据集为$$X_{mxn}$$ ,我们的目标是找到最佳的投影空间$$W_{nxk}=(w_1, w_2, …, w_k)$$,其中$$w_i$$是单位向量且$$w_i$$与$$w_j (i\neq j)$$正交， 何为最佳的W就是原始样本点投影到W上之后，使得投影后的样本点方差最大。

由于投影后均值为0，因此投影后的总方差为：
$$
\frac{1}{m}\sum_{i=1}^m (x^{(i)T}w)^2 = \frac{1}{m}\sum_{i=1}^m w^T x^{(i)} x^{(i)T}w =\sum_{i=1}^m w^T (\frac{1}{m} x^{(i)} x^{(i)T}) w_1
$$
$$\frac{1}{m} x^{(i)} x^{(i)T}$$是原始数据集XX的协方差矩阵。

记$$\lambda =  \frac{1}{m}\sum_{i=1}^m (x^{(i)T}w)^2$$,$$\sum = \frac{1}{m} x^{(i)} x^{(i)T}$$,则有：
$$
\lambda = w^T \sum w
$$
上式两边同时左乘$$w$$，注意到$$ww^T=1$$(单位向量)，则有:
$$
\lambda w = \sum w
$$
所以$$w$$是矩阵$$∑$$的特征值所对应的特征向量。

欲使投影后的总方差最大，即$$λ$$最大，因此最佳的投影向量$$w$$是特征值$$λ$$最大时对应的特征向量，因此，当我们将$$w$$设置为与具有最大的特征值$$λ$$的特征向量相等时，方差会达到最大值。这个特征向量被称为第一主成分。

#### PCA算法流程

算法输入：数据集$$X_{mxn}$$

* 按列计算数据集$$X$$的均值$$X_{mean}$$，然后令$$X_{new}$$=$$X$$−$$X_{mean}$$；
* 求解矩阵$$X_{new}$$的协方差矩阵，并将其记为$$Cov$$；
* 计算协方差矩阵$$Cov$$的特征值和相应的特征向量； 
* 将特征值按照从大到小的排序，选择其中最大的$$k$$个，然后将其对应的$$k$$个特征向量分别作为列向量组成特征向量矩阵$$W_{nxk}$$;
* 计算$$X_{new}W$$，即将数据集$$X_{new}$$投影到选取的特征向量上，这样就得到了我们需要的已经降维的数据集$$X_{new}W$$。

### 最小平方误差理论

![](https://github.com/benkang-chen/algorithm/blob/master/algorithm_by_python/picture/pca2.png)

如上图所示，假设有这样的二维样本点（红色点），按照前文我们讲解的最大方差理论，我们的目标是是求一条直线，使得样本点投影到直线或者平面上的点的方差最大。本质是求直线或者平面，那么度量直线求的好不好，不仅仅只有方差最大化的方法。再回想我们最开始学习的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。比如这个问题中，特征是x，类标签是y。回归时最小二乘法度量的是距离d。如果使用回归方法来度量最佳直线，那么就是直接在原始样本上做回归了，跟特征选择就没什么关系了。

因此，我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离$$d′$$来度量。

现在有$$m$$个样本点$$x^{(1)}, ..., x^{(m)}$$,每个样本点为$$n$$维。将样本点$$x^{(i)}$$在直线上的投影记为$$x^{(1)’}$$,那么我们就是要最小化
$$
\sum_{i=1}^{m}(x^{(i)’} - x^{(i)})^2
$$
这个公式称作最小平方误差（Least Squared Error）。

首先，我们确定直线经过的点，假设要在空间中找一点$$x_0$$来代表这$$m$$个样本点，“代表”这个词不是量化的，因此要量化的话，我们就是要找一个$$n$$维的点$$x_0$$，使得
$$
J_0(x_0) = \sum_{i=1}^{m}(x_0 - x^{(i)})^2
$$
最小。其中$$J_0(x_0)$$是平方错误评价函数(squared-error criterion function)，假设$$\bar x$$为$$m$$个样本点的均值，即
$$
\bar x = \frac{1}{m} \sum_{i=1}^{m}x^{(i)}
$$
则
$$
\begin{align*}
J_0(x_0) 
& = \sum_{i=1}^{m}(x_0 - x^{(i)})^2 \\
& = \sum_{i=1}^{m}((x_0 - \bar x) - (x^{(i)} - \bar x))^2 \\
& = \sum_{i=1}^{m}(x_0 - \bar x)^2 - 2 \sum_{i=1}^{m}(x_0 - \bar x)^T (x^{(i)} - \bar x) +  \sum_{i=1}^{m}(x^{(i)} - \bar x)^2 \\
& = \sum_{i=1}^{m}(x_0 - \bar x)^2 - 2 (x_0 - \bar x)^T \sum_{i=1}^{m} (x^{(i)} - \bar x) +  \sum_{i=1}^{m}(x^{(i)} - \bar x)^2  \\
& = \sum_{i=1}^{m}(x_0 - \bar x)^2  +  \sum_{i=1}^{m}(x^{(i)} - \bar x)^2  
\end{align*}
$$
显然，上式的第二项与$$x_0$$无关，因此，$$J_0(x_0)$$在$$\bar x$$处有最小值。

接下来，我们确定直线的方向向量。我们已经知道直线经过点$$\bar x$$，假设直线的方向是单位向量$$\vec e$$。那么直线上任意一点$$x^{(i)'}$$有：
$$
x^{(i)’} = \bar x + a_i \vec e
$$
其中，$$a_i$$是$$x^{(i)′}$$到点$$\bar x$$的距离。

重新定义最小平方误差：
$$
\begin{align*}
J_1(a_1, a_2, ..., a_m, \vec e) 
& = \sum_{i=1}^{m}(x^{(i)’} - x^{(i)})^2 \\
& = \sum_{i=1}^{m}((\bar x + a_i \vec e) - x^{(i)})^2 \\
& = \sum_{i=1}^{m}(a_i \vec e - (x^{(i)} - \bar x ))^2 \\
& = \sum_{i=1}^{m} a^2_i \vec e^2 -2 \sum_{i=1}^{m} a_i \vec e^T (x^{(i)}-\bar x) + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2 
\end{align*}
$$
我们首先固定$$\vec e$$，将其看做是常量，然后令$$J_1$$ 关于 $$a_i$$ 的导数等于0，则有：
$$
a_i = \vec e^T(x^{(i)}-\bar x),
$$
这个结果意思是说，如果知道了$$\vec e$$，那么将$$(x_{(i)}−\bar x)$$与$$\vec e$$内积，就可以知道了$$x_{(i)}$$在$$\vec e$$上的投影离$$\bar x$$的长度距离，不过这个结果不用求都知道。

然后是固定$$a_i$$，对$$\vec e$$偏导数，我们先将$$ a_i$$代入$$J_1$$，得
$$
\begin{align*}
J_1( \vec e) 
& = \sum_{i=1}^{m} a^2_i \vec e^2 -2 \sum_{i=1}^{m} a_i \vec e^T (x^{(i)}-\bar x) + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2  \\
& = \sum_{i=1}^{m} a^2_i \vec e^2 -2 \sum_{i=1}^{m} a^2_i+ \sum_{i=1}^{m} (x^{(i)} - \bar x)^2  \\
& = - \sum_{i=1}^{m} (\vec e^T(x^{(i)}-\bar x))^2 + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2 \\
& = - \sum_{i=1}^{m} \vec e^T (x^{(i)}-\bar x)(x^{(i)}-\bar x)^T \vec e + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2 \\
& = -  \vec e^T S \vec e + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2 
\end{align*}
$$
其中$$S = \sum_{i=1}^{m} (x^{(i)}-\bar x)(x^{(i)}-\bar x)^T$$与协方差矩阵类似,只是缺少个分母$$n−1$$，我们称之为散列矩阵(scatter matrix)。

现在我们就可以用拉格朗日乘数法求解方向向量$$\vec e$$了。令
$$
f(\vec e) = - \vec e^T S \vec e + \sum_{i=1}^{m} (x^{(i)} - \bar x)^2 + \lambda (\vec e^T \vec e - 1)
$$
令上式关于$$\vec e$$的偏导数等于0，则可得
$$
S \vec e = \lambda \vec e
$$
两边除以$$n−1$$就变成了对协方差矩阵求特征值向量了。

从不同的思路出发，最后得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：

![](https://github.com/benkang-chen/algorithm/blob/master/algorithm_by_python/picture/pca3.png)

这时候点都聚集在新的坐标轴周围，因为我们使用的最小平方误差的意义就在此。

## PCA算法优缺点：

优点：

- 它是无监督学习，完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。
- 用PCA技术可以对数据进行降维，同时对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。
- 各主成分之间正交，可消除原始数据成分间的相互影响。
- 计算方法简单，易于在计算机上实现。

缺点：

- 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。
- 贡献率小的主成分往往可能含有对样本差异的重要信息。
- 特征值矩阵的正交向量空间是否唯一有待讨论。
- 在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的，此时在寻找主元时不能将方差作为衡量重要性的标准。