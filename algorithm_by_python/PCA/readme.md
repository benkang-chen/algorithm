## PCA算法介绍

主成分分析（Principal Components Analysis），简称PCA，是一种数据降维技术，用于数据预处理。一般我们获取的原始数据维度都很高，比如1000个特征，在这1000个特征中可能包含了很多无用的信息或者噪声，真正有用的特征才100个，那么我们可以运用PCA算法将1000个特征降到100个特征。这样不仅可以去除无用的噪声，还能减少很大的计算量。

### PCA算法是如何实现的？

简单来说，就是将数据从原始的空间中转换到新的特征空间中，例如原始的空间是三维的(x,y,z)，x、y、z分别是原始空间的三个基，我们可以通过某种方法，用新的坐标系(a,b,c)来表示原始的数据，那么a、b、c就是新的基，它们组成新的特征空间。在新的特征空间中，可能所有的数据在c上的投影都接近于0，即可以忽略，那么我们就可以直接用(a,b)来表示数据，这样数据就从三维的(x,y,z)降到了二维的(a,b)。

### 问题是如何求新的基(a,b,c)?

一般步骤是这样的：先对原始数据零均值化，然后求协方差矩阵，接着对协方差矩阵求特征向量和特征值，这些特征向量组成了新的特征空间。参考链接http://deeplearning.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90

## 衡量投影的方法

### 1.最大方差理论

​	在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的k维特征是将n维样本点变换为k维后，每一维上的样本方差都尽可能的大。

​	首先，考虑在一维空间 (k=1) 上的投影。我们可以使用 n 维向量u定义这个空间的方向。为了方便(并且不失一般性)，我们假定选择一个单位向量，从而  
$$
u^T u = 1
$$
（注意，我们只对$$ u $$的方向感兴趣，而对 $$u$$本身的大小不感兴趣)。

![pca](https://github.com/benkang-chen/algorithm/blob/master/algorithm_by_python/picture/pca1.png)




